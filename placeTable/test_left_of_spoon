#!/usr/bin/env python2


'''This code tests out using rbfs around each object. The correct reward is to place it to the left of a the spoon (object 0)'''

import random
import numpy as np
import birl
import active_utils as autils
import matplotlib.pyplot as plt
import active_var_complexreward as active_var
import sys


from copy import deepcopy



def test_placements(true_reward, num_test):
    test_rbfs = []
    for i in range(num_test):
        #generate new centers, but keep weights
        num_objs = true_reward.num_objects
        new_centers = np.random.rand(num_objs, 2)
        obj_weights = true_reward.obj_weights.copy()
        abs_weights = true_reward.abs_weights.copy()
        new_rbf = autils.RbfComplexReward(new_centers, obj_weights, abs_weights)
        test_rbfs.append(new_rbf)
        #autils.visualize_reward(new_rbf, "test placement")
    return test_rbfs


#return mean and standard deviation of loss over test placements
def calc_test_reward_loss(test_placements, map_params, visualize=False):
    losses = []
    for placement in test_placements:
        test_config = placement.obj_centers
        true_params = (placement.obj_weights, placement.abs_weights)
        ploss = active_var.calculate_policy_loss(test_config, true_params, map_params)
        if visualize:
            test_map = autils.RbfComplexReward(test_config, map_params[0], map_params[1])
            autils.visualize_reward(test_map, "testing with map reward")
            plt.show()
        losses.append(ploss)
    losses = np.array(losses)
    return np.mean(losses), np.std(losses), np.max(losses)


def calc_test_placement_loss(test_placements, map_params, visualize=False):
    losses = []
    cnt = 0
    for placement in test_placements:
        #print cnt
        cnt += 1
        test_config = placement.obj_centers
        true_params = (placement.obj_weights, placement.abs_weights)
        ploss = active_var.calculate_placement_loss(test_config, true_params, map_params)
        if visualize:
            test_map = autils.RbfComplexReward(test_config, map_params[0], map_params[1])
            autils.visualize_reward(test_map, "testing with map reward")
            plt.show()
        losses.append(ploss)
    losses = np.array(losses)
    return np.mean(losses), np.std(losses), np.max(losses)


if __name__=="__main__":

    rand_seed = 12345
    np.random.seed(rand_seed)

    beta=100.0
    num_steps = 1000
    step_std = 0.05
    burn = 0
    skip = 25

    num_objects = 3 #first one is spoon, others are distractors
    #object weights are for center, top left, top right, bottom left, bottom right
    obj0_weights = np.array([0.0, 0.5, 0.0, 0.5, 0.0]) #spoon: equal weight on top left and bottom left rbf results in placement directly to left of object
    obj1_weights = np.array([0.0, 0.0, 0.0, 0.0, 0.0]) #distractor
    obj2_weights = np.array([0.0, 0.0, 0.0, 0.0, 0.0]) #distractor
    obj_weights = np.concatenate((obj0_weights, obj1_weights, obj2_weights))
    abs_weights = np.array([0.0, 0.0, 0.0,
                            0.0, 0.0, 0.0,
                            0.0, 0.0, 0.0]) # no absolute placement preferences

    num_obj_weights = len(obj_weights)
    num_abs_weights = len(abs_weights)

    birl = birl.BIRL(num_obj_weights, num_abs_weights, beta, num_steps, step_std, burn, skip)

    #give three demos in different positions
    num_demos = 3
    for i in range(num_demos):
        #generate random object placements
        obj_centers = np.random.random((num_objects,2))
        demo_rbf = autils.RbfComplexReward(obj_centers, obj_weights, abs_weights)
        best_x, reward = demo_rbf.estimate_best_placement()
        birl.add_demonstration(demo_rbf.obj_centers, best_x)

        print "demo", best_x
        autils.visualize_reward(demo_rbf, "demo {} and ground truth reward".format(i) )
    #plt.show()


    #run birl to get MAP estimate
    birl.run_inference()


    #print out the map reward weights
    map_obj_wts, map_abs_wts = birl.get_map_params()
    print "obj weights", map_obj_wts
    print "abs weights", map_abs_wts

    #create rbf for MAP reward found by BIRL in test configurations to see how it generalizes
    for i in range(3):
        #I'm giving it random object placements to see if it generalizes
        rand_obj_centers = np.random.random((num_objects,2))
        map_reward_rbf = autils.RbfComplexReward(rand_obj_centers, map_obj_wts, map_abs_wts)

        #visualize learned reward and optimal placement
        autils.visualize_reward(map_reward_rbf, title="test {} for MAP reward".format(i))
    plt.show()
